config = {
    'duo_attention': {  # only works for Llama-3.1-8B-Instruct, which the DuoAttention repo supports
        'algorithm': 'duo_attention',
        'on_the_fly_scoring': False,
    },
}
